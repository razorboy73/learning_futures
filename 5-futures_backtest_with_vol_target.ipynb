{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c12b1490",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: c:\\TWS API\\source\\pythonclient\\TradingIdeas\\Futures\n",
      "Expected files in: C:\\TWS API\\source\\pythonclient\\TradingIdeas\\Futures\\04a-risk_volume\\norgate_continuous\\parquet\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(\"Current working directory:\", os.getcwd())\n",
    "print(\"Expected files in:\", Path(\"./04a-risk_volume/norgate_continuous/parquet\").resolve())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6b7b59cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory exists: True\n",
      "Number of files: 105\n",
      "First 5 files: ['6A.parquet', '6B.parquet', '6C.parquet', '6E.parquet', '6J.parquet']\n",
      "\n",
      "Test file: 6A.parquet\n",
      "Has usd_risk_volume: True\n",
      "Columns: ['date', 'open', 'high', 'low', 'close', 'volume', 'delivery month', 'open interest', 'daily_prices_change_pts', 'daily_prices_change_percent', 'daily_std_full', 'annualized_std_full', 'ewma32_std', 'rolling_avg_volume_60d', 'rolling_avg_volume_20d', 'annualized_vol', 'usd_risk_volume']\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Check if directory exists\n",
    "dir_path = Path(\"./04a-risk_volume/norgate_continuous/parquet\")\n",
    "print(f\"Directory exists: {dir_path.exists()}\")\n",
    "\n",
    "# List files\n",
    "if dir_path.exists():\n",
    "    files = list(dir_path.glob(\"*.parquet\"))\n",
    "    print(f\"Number of files: {len(files)}\")\n",
    "    print(f\"First 5 files: {[f.name for f in files[:5]]}\")\n",
    "    \n",
    "    # Test loading one file\n",
    "    if files:\n",
    "        test_file = files[0]\n",
    "        import pandas as pd\n",
    "        df = pd.read_parquet(test_file)\n",
    "        print(f\"\\nTest file: {test_file.name}\")\n",
    "        print(f\"Has usd_risk_volume: {'usd_risk_volume' in df.columns}\")\n",
    "        print(f\"Columns: {df.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1b99ea8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After reading parquet:\n",
      "  Columns: ['date', 'open', 'high', 'low', 'close', 'volume', 'delivery month', 'open interest', 'daily_prices_change_pts', 'daily_prices_change_percent', 'daily_std_full', 'annualized_std_full', 'ewma32_std', 'rolling_avg_volume_60d', 'rolling_avg_volume_20d', 'annualized_vol', 'usd_risk_volume']\n",
      "  Has usd_risk_volume: True\n",
      "After selecting columns [date, close, ewma32_std]:\n",
      "  Columns: ['date', 'close', 'ewma32_std']\n",
      "  Has usd_risk_volume: False\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# Simulate what the backtest does\n",
    "def load_instrument_df(filepath, date_col, close_col, vol_col):\n",
    "    \"\"\"Mimic the backtest loading function\"\"\"\n",
    "    df = pd.read_parquet(filepath)\n",
    "    \n",
    "    print(f\"After reading parquet:\")\n",
    "    print(f\"  Columns: {df.columns.tolist()}\")\n",
    "    print(f\"  Has usd_risk_volume: {'usd_risk_volume' in df.columns}\")\n",
    "    \n",
    "    # Check if there's any column filtering happening\n",
    "    df = df[[date_col, close_col, vol_col]]  # This might be the problem!\n",
    "    \n",
    "    print(f\"After selecting columns [{date_col}, {close_col}, {vol_col}]:\")\n",
    "    print(f\"  Columns: {df.columns.tolist()}\")\n",
    "    print(f\"  Has usd_risk_volume: {'usd_risk_volume' in df.columns}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Test it\n",
    "test_file = Path(\"./04a-risk_volume/norgate_continuous/parquet/6A.parquet\")\n",
    "df = load_instrument_df(test_file, \"date\", \"close\", \"ewma32_std\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b1d9a409",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Micro mappings defined\n",
      "‚úì Micro mappings defined\n",
      "  Norgate micros: 6\n",
      "  Broker-only micros: 19\n",
      "   signal_symbol trade_symbol  multiplier_used multiplier_source\n",
      "0             6A          M6A         100000.0     signal_symbol\n",
      "1             6B          M6B          62500.0     signal_symbol\n",
      "2             6C          MCD         100000.0     signal_symbol\n",
      "3             6E          M6E         125000.0     signal_symbol\n",
      "4             6J          MJY         125000.0     signal_symbol\n",
      "..           ...          ...              ...               ...\n",
      "94            ZQ           ZQ           4167.0      trade_symbol\n",
      "95            ZR           ZR             20.0      trade_symbol\n",
      "96            ZS          MZS             50.0     signal_symbol\n",
      "97            ZT           ZT           2000.0      trade_symbol\n",
      "98            ZW          MZW             50.0     signal_symbol\n",
      "\n",
      "[99 rows x 4 columns]\n",
      "Wrote outputs to: C:\\TWS API\\source\\pythonclient\\TradingIdeas\\Futures\\05-futures_backtest_position_sizing_mapped\n",
      "- portfolio_equity.csv, portfolio_equity.parquet\n",
      "- positions.csv, positions.parquet\n",
      "- trades.csv\n",
      "- instrument_map.csv\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\"\"\"\n",
    "===============================================================================\n",
    "FUTURES BACKTEST ENGINE WITH VOL-TARGET POSITION SIZING (EWMA32)\n",
    "WITH MICRO/MINI TRADE MAPPING\n",
    "===============================================================================\n",
    "\n",
    "Signals/vol/price:\n",
    "- Always computed from FULL-SIZE \"signal symbol\" data files.\n",
    "\n",
    "Sizing + P&L:\n",
    "- Use MICRO/MINI \"trade symbol\" multiplier (point_value) when available per mapping.\n",
    "- Price changes are still taken from the signal symbol (assumes micro tracks underlying).\n",
    "\n",
    "Position sizing:\n",
    "\n",
    "    N_t = floor( Equity_t * tau_i / (Multiplier_trade * Price_signal_t * FX * sigma_ann_t) )\n",
    "\n",
    "Where:\n",
    "- sigma_ann_t = ewma32_std_daily * sqrt(256)\n",
    "- tau_i = target_portfolio_vol / K_t (equal risk split across active instruments)\n",
    "- FX configurable, default 1\n",
    "\n",
    "P&L:\n",
    "    pnl_t = sum_i contracts_{i,t-1} * multiplier_trade_i * FX * (close_signal_t - close_signal_{t-1})\n",
    "\n",
    "Outputs:\n",
    "- portfolio_equity.(csv|parquet)\n",
    "- positions.(csv|parquet)              # positions expressed in TRADE contracts (e.g., MES)\n",
    "- trades.csv                            # contract changes in TRADE contracts\n",
    "- instrument_map.csv                    # signal_symbol -> trade_symbol -> multiplier\n",
    "\"\"\"\n",
    "\n",
    "# ============================================================\n",
    "# Configuration\n",
    "# ============================================================\n",
    "\n",
    "# ==================== MICRO/MINI CONTRACT MAPPING ====================\n",
    "# Maps full-size signal symbol -> micro/mini trade symbol\n",
    "# Only includes mappings where the micro/mini EXISTS and is commonly traded\n",
    "\n",
    "MICRO_MINI_MAPPING = {\n",
    "    # Equity Indices - Micros available in Norgate\n",
    "    'ES': 'MES',    # E-mini S&P 500 -> Micro E-mini S&P 500\n",
    "    'NQ': 'MNQ',    # E-mini Nasdaq -> Micro E-mini Nasdaq\n",
    "    'RTY': 'M2K',   # E-mini Russell 2000 -> Micro E-mini Russell\n",
    "    'YM': 'MYM',    # E-mini Dow -> Micro E-mini Dow\n",
    "\n",
    "    # Crypto - Micros available in Norgate\n",
    "    'BTC': 'MBT',   # Bitcoin -> Micro Bitcoin\n",
    "    'ETH': 'MET',   # Ether -> Micro Ether\n",
    "}\n",
    "\n",
    "# Additional mappings for contracts where micro EXISTS at broker but NOT in Norgate\n",
    "# These use full-size data for signals but micro specs for sizing/P&L\n",
    "BROKER_ONLY_MICROS = {\n",
    "    # Currencies\n",
    "    '6A': 'M6A',\n",
    "    '6B': 'M6B',\n",
    "    '6C': 'MCD',\n",
    "    '6E': 'M6E',\n",
    "    '6J': 'MJY',\n",
    "    '6S': 'MSF',\n",
    "\n",
    "    # Energy\n",
    "    'CL': 'MCL',\n",
    "    'NG': 'MNG',\n",
    "    'RB': 'MRB',\n",
    "    'HO': 'MHO',\n",
    "\n",
    "    # Metals\n",
    "    'GC': 'MGC',\n",
    "    'SI': 'SIL',\n",
    "    'HG': 'MHG',\n",
    "\n",
    "    # Agriculture\n",
    "    'ZC': 'MZC',\n",
    "    'ZW': 'MZW',\n",
    "    'ZS': 'MZS',\n",
    "    'ZM': 'MZM',\n",
    "    'ZL': 'MZL',\n",
    "\n",
    "    # Volatility\n",
    "    'VX': 'VXM',\n",
    "}\n",
    "\n",
    "# Inverse map: micro/mini trade symbol -> full-size signal symbol\n",
    "# Used to ensure we do NOT accidentally use micro files (e.g., MES.parquet) as signal sources\n",
    "\n",
    "MICRO_TO_SIGNAL = {v: k for k, v in MICRO_MINI_MAPPING.items()}\n",
    "\n",
    "print(f\"‚úì Micro mappings defined\")\n",
    "\n",
    "CONFIG = {\n",
    "    # Limit trading universe by TRADE symbol (e.g., {\"MES\"} to trade only Micro ES)\n",
    "    #\"trade_symbol_allowlist\": {\"MES\"},  # set to None to disable filtering\n",
    "    \"trade_symbol_allowlist\": None,  # set to None to disable filtering\n",
    "\n",
    "    # Backtest range (None means use all available)\n",
    "    \"start_date\": \"2015-01-01\",\n",
    "    \"end_date\": None,\n",
    " \n",
    "    # Capital + risk target\n",
    "    \"initial_capital\": 400_000.0,\n",
    "    \"target_portfolio_vol\": 0.20,     # 20% annualized target (portfolio-level)\n",
    "    \"trading_days_per_year\": 256,\n",
    "    \"fx\": 1.0,\n",
    "    \n",
    "    # NEW: USD Risk Volume Filtering\n",
    "    \"min_usd_risk_volume\": 0,  # Minimum USD risk volume to include instrument (0 = all instruments)\n",
    "                                # Example: 1_000_000 = only instruments with >$1M daily USD risk\n",
    "                                # Example: 50_000_000 = only instruments with >$50M daily USD risk\n",
    "\n",
    "    \n",
    "\n",
    "    # Signal params (trend strategy)\n",
    "    \"fast_sma\": 50,\n",
    "    \"slow_sma\": 100,\n",
    "\n",
    "    # Costs (optional)\n",
    "    \"commission_per_contract\": 2.50,  # per trade contract changed\n",
    "    \"apply_commissions\": True,\n",
    "\n",
    "    # Data locations\n",
    "    \"contracts_file\": Path(\"./01-futures_universe/futures_contracts_full.parquet\"),\n",
    "\n",
    "    # IMPORTANT:\n",
    "    # This directory contains the FULL-SIZE signal symbol data with ewma32_std and close\n",
    "    \"signal_vol_prices_dir\": Path(\"./04a-risk_volume/norgate_continuous/parquet\"),\n",
    "    \n",
    "    # Column names in per-instrument files\n",
    "    \"date_col\": \"date\",\n",
    "    \"close_col\": \"close\",\n",
    "    \"vol_col\": \"ewma32_std\",  # DAILY EWMA stdev of returns (starts after 32 days in your pipeline)\n",
    "\n",
    "    # Output root\n",
    "    \"output_root\": Path(\"./05-futures_backtest_position_sizing_mapped\"),\n",
    "\n",
    "    # Behavior\n",
    "    \"require_trade_multiplier\": True,  # if True, error if mapped trade symbol has no multiplier\n",
    "    \"default_to_signal_multiplier_if_missing\": True,  # if trade multiplier missing, use signal multiplier (if allowed)\n",
    "}\n",
    "\n",
    "print(f\"‚úì Micro mappings defined\")\n",
    "print(f\"  Norgate micros: {len(MICRO_MINI_MAPPING)}\")\n",
    "print(f\"  Broker-only micros: {len(BROKER_ONLY_MICROS)}\")\n",
    "\n",
    "# ============================================================\n",
    "# Helpers\n",
    "# ============================================================\n",
    "\n",
    "def load_contract_multipliers(contracts_path: Path) -> dict:\n",
    "    \"\"\"\n",
    "    Load point_value multipliers from futures_contracts_full.parquet.\n",
    "    Returns {symbol: point_value}.\n",
    "    Tries to infer column names robustly.\n",
    "    \"\"\"\n",
    "    if not contracts_path.exists():\n",
    "        raise FileNotFoundError(f\"Contracts file not found: {contracts_path}\")\n",
    "\n",
    "    df = pd.read_parquet(contracts_path)\n",
    "    df.columns = [str(c).strip().lower() for c in df.columns]\n",
    "\n",
    "    sym_col_candidates = [\"symbol\", \"ticker\", \"root\", \"contract\", \"instrument\"]\n",
    "    sym_col = next((c for c in sym_col_candidates if c in df.columns), None)\n",
    "    if sym_col is None:\n",
    "        raise ValueError(f\"Could not find a symbol column in contracts file. Columns: {list(df.columns)}\")\n",
    "\n",
    "    pv_col_candidates = [\"point_value\", \"pointvalue\", \"multiplier\", \"contract_multiplier\"]\n",
    "    pv_col = next((c for c in pv_col_candidates if c in df.columns), None)\n",
    "    if pv_col is None:\n",
    "        raise ValueError(f\"Could not find a point_value column in contracts file. Columns: {list(df.columns)}\")\n",
    "\n",
    "    out = {}\n",
    "    for _, r in df.iterrows():\n",
    "        sym = str(r[sym_col]).strip().upper()\n",
    "        if not sym or sym.lower() == \"nan\":\n",
    "            continue\n",
    "        pv = pd.to_numeric(r[pv_col], errors=\"coerce\")\n",
    "        if pd.isna(pv) or pv <= 0:\n",
    "            continue\n",
    "        out[sym] = float(pv)\n",
    "\n",
    "    if not out:\n",
    "        raise RuntimeError(\"No valid {symbol: point_value} rows found in contracts file.\")\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def load_instrument_df(path: Path, date_col: str, close_col: str, vol_col: str) -> pd.DataFrame:\n",
    "    df = pd.read_parquet(path)\n",
    "    df.columns = [str(c).strip().lower() for c in df.columns]\n",
    "\n",
    "    if date_col not in df.columns:\n",
    "        if isinstance(df.index, pd.DatetimeIndex):\n",
    "            df = df.reset_index().rename(columns={\"index\": date_col})\n",
    "        else:\n",
    "            raise ValueError(f\"Missing '{date_col}' in {path.name}. Columns: {list(df.columns)}\")\n",
    "\n",
    "    df[date_col] = pd.to_datetime(df[date_col], errors=\"coerce\")\n",
    "    df = df.dropna(subset=[date_col]).sort_values(date_col)\n",
    "\n",
    "    df[close_col] = pd.to_numeric(df[close_col], errors=\"coerce\")\n",
    "    df[vol_col] = pd.to_numeric(df[vol_col], errors=\"coerce\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def compute_signals(close: pd.Series, fast: int, slow: int) -> pd.Series:\n",
    "    fast_ma = close.rolling(fast).mean()\n",
    "    slow_ma = close.rolling(slow).mean()\n",
    "    sig = pd.Series(0, index=close.index, dtype=int)\n",
    "    sig[fast_ma > slow_ma] = 1\n",
    "    sig[fast_ma < slow_ma] = -1\n",
    "    return sig\n",
    "\n",
    "\n",
    "def safe_floor(x: float) -> int:\n",
    "    if not np.isfinite(x) or x <= 0:\n",
    "        return 0\n",
    "    return int(np.floor(x))\n",
    "\n",
    "\n",
    "def map_trade_symbol(signal_symbol: str) -> str:\n",
    "    \"\"\"\n",
    "    Map full-size signal symbol to trade symbol (micro/mini if available).\n",
    "    Falls back to signal symbol if no mapping exists.\n",
    "    \"\"\"\n",
    "    s = signal_symbol.upper()\n",
    "    if s in MICRO_MINI_MAPPING:\n",
    "        return MICRO_MINI_MAPPING[s]\n",
    "    if s in BROKER_ONLY_MICROS:\n",
    "        return BROKER_ONLY_MICROS[s]\n",
    "    return s\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Backtest Engine\n",
    "# ============================================================\n",
    "\n",
    "def run_backtest(cfg: dict) -> tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    date_col = cfg[\"date_col\"]\n",
    "    close_col = cfg[\"close_col\"]\n",
    "    vol_col = cfg[\"vol_col\"]\n",
    "\n",
    "    multipliers = load_contract_multipliers(cfg[\"contracts_file\"])\n",
    "\n",
    "    signal_dir: Path = cfg[\"signal_vol_prices_dir\"]\n",
    "    if not signal_dir.exists():\n",
    "        raise FileNotFoundError(f\"Signal vol/price parquet dir not found: {signal_dir}\")\n",
    "\n",
    "    signal_files = sorted(signal_dir.glob(\"*.parquet\"))\n",
    "    if not signal_files:\n",
    "        raise RuntimeError(f\"No signal parquet files found in: {signal_dir}\")\n",
    "\n",
    "    # Load signals from full-size files; map to trade symbol for multiplier + P&L sizing\n",
    "    instruments = []\n",
    "    data_map = {}\n",
    "    instrument_meta_rows = []\n",
    "\n",
    "    for p in signal_files:\n",
    "        signal_sym = p.stem.strip().upper()\n",
    "\n",
    "        # SAFEGUARD: If this file is a micro/mini symbol (e.g., MES) that maps back to a full-size\n",
    "        # signal symbol (e.g., ES), skip it as a signal source. We want full-size data for signals.\n",
    "        if signal_sym in MICRO_TO_SIGNAL:\n",
    "            # Example: signal_sym == \"MES\" -> skip, we'll use \"ES\" file instead\n",
    "            continue\n",
    "        trade_sym = map_trade_symbol(signal_sym)\n",
    "\n",
    "        # OPTION 1: Filter by TRADE symbol allowlist (e.g., {\"MES\"}).\n",
    "        # OPTION 1: Filter by TRADE symbol allowlist (e.g., {\"MES\"}).\n",
    "        allow = cfg.get(\"trade_symbol_allowlist\")\n",
    "        if allow is not None and trade_sym not in set(allow):\n",
    "            continue\n",
    "\n",
    "        # Need signal data (close, ewma32, usd_risk_volume)\n",
    "        df = load_instrument_df(p, date_col, close_col, vol_col)\n",
    "        \n",
    "        # OPTION 2: Filter by USD risk volume (NEW)\n",
    "        min_usd_risk = cfg.get(\"min_usd_risk_volume\", 0)\n",
    "        if min_usd_risk > 0:\n",
    "            # Check if usd_risk_volume column exists\n",
    "            if \"usd_risk_volume\" not in df.columns:\n",
    "                print(f\"  ‚ö†Ô∏è  Warning: {signal_sym} missing 'usd_risk_volume' column, skipping\")\n",
    "                print(f\"     Make sure you're reading from 04a-risk_volume directory\")\n",
    "                continue\n",
    "            \n",
    "            # Get the most recent USD risk value (last non-NaN value)\n",
    "            usd_risk_series = df[\"usd_risk_volume\"].dropna()\n",
    "            if len(usd_risk_series) == 0:\n",
    "                print(f\"  ‚ö†Ô∏è  {signal_sym}: No valid USD risk data, skipping\")\n",
    "                continue\n",
    "            \n",
    "            recent_usd_risk = float(usd_risk_series.iloc[-1])\n",
    "            \n",
    "            # Filter out if below minimum\n",
    "            if recent_usd_risk < min_usd_risk:\n",
    "                print(f\"  üö´ {signal_sym} (trades as {trade_sym}): \"\n",
    "                      f\"USD risk ${recent_usd_risk:,.0f} < min ${min_usd_risk:,.0f} - FILTERED OUT\")\n",
    "                continue\n",
    "            else:\n",
    "                print(f\"  ‚úÖ {signal_sym} (trades as {trade_sym}): \"\n",
    "                      f\"USD risk ${recent_usd_risk:,.0f} >= min ${min_usd_risk:,.0f} - INCLUDED\")\n",
    "        \n",
    "        df = df.set_index(date_col)\n",
    "        df[\"signal\"] = compute_signals(df[close_col], cfg[\"fast_sma\"], cfg[\"slow_sma\"])\n",
    "\n",
    "        # Determine multiplier to use (trade symbol preferred)\n",
    "        trade_mult = multipliers.get(trade_sym)\n",
    "        signal_mult = multipliers.get(signal_sym)\n",
    "\n",
    "        chosen_mult = trade_mult\n",
    "\n",
    "        if chosen_mult is None:\n",
    "            if cfg.get(\"default_to_signal_multiplier_if_missing\", True) and signal_mult is not None:\n",
    "                chosen_mult = signal_mult\n",
    "            elif cfg.get(\"require_trade_multiplier\", True):\n",
    "                raise RuntimeError(\n",
    "                    f\"Missing multiplier for trade symbol '{trade_sym}' (mapped from '{signal_sym}'). \"\n",
    "                    f\"Also signal multiplier exists? {signal_mult is not None}.\"\n",
    "                )\n",
    "\n",
    "        if chosen_mult is None:\n",
    "            # if both missing, skip\n",
    "            continue\n",
    "\n",
    "        data_map[signal_sym] = df\n",
    "        instruments.append(signal_sym)\n",
    "\n",
    "        instrument_meta_rows.append({\n",
    "            \"signal_symbol\": signal_sym,\n",
    "            \"trade_symbol\": trade_sym,\n",
    "            \"multiplier_used\": float(chosen_mult),\n",
    "            \"multiplier_source\": \"trade_symbol\" if trade_mult is not None else (\"signal_symbol\" if signal_mult is not None else \"missing\"),\n",
    "        })\n",
    "\n",
    "    if not instruments:\n",
    "        raise RuntimeError(\n",
    "            \"No instruments loaded. Check that your signal files exist and multipliers are available.\"\n",
    "        )\n",
    "\n",
    "    instrument_map_df = pd.DataFrame(instrument_meta_rows).sort_values([\"signal_symbol\", \"trade_symbol\"])\n",
    "    print(instrument_map_df)\n",
    "    # Build common calendar (intersection across signal symbols)\n",
    "    common_dates = None\n",
    "    for sym in instruments:\n",
    "        idx = data_map[sym].index\n",
    "        common_dates = idx if common_dates is None else common_dates.intersection(idx)\n",
    "    common_dates = common_dates.sort_values()\n",
    "\n",
    "    if cfg[\"start_date\"]:\n",
    "        common_dates = common_dates[common_dates >= pd.Timestamp(cfg[\"start_date\"])]\n",
    "    if cfg[\"end_date\"]:\n",
    "        common_dates = common_dates[common_dates <= pd.Timestamp(cfg[\"end_date\"])]\n",
    "\n",
    "    if len(common_dates) < 3:\n",
    "        raise RuntimeError(\"Not enough common dates after filtering to run the backtest.\")\n",
    "\n",
    "    # Matrices\n",
    "    close = pd.DataFrame({sym: data_map[sym].reindex(common_dates)[close_col] for sym in instruments})\n",
    "    vol_d = pd.DataFrame({sym: data_map[sym].reindex(common_dates)[vol_col] for sym in instruments})\n",
    "    sig = pd.DataFrame({sym: data_map[sym].reindex(common_dates)[\"signal\"] for sym in instruments}).astype(int)\n",
    "\n",
    "    # Annualize daily EWMA vol\n",
    "    vol_ann = vol_d * np.sqrt(cfg[\"trading_days_per_year\"])\n",
    "\n",
    "    # Multiplier per signal symbol is actually the \"trade multiplier used\"\n",
    "    mult_used = {}\n",
    "    trade_symbol_used = {}\n",
    "    for _, r in instrument_map_df.iterrows():\n",
    "        s = r[\"signal_symbol\"]\n",
    "        mult_used[s] = float(r[\"multiplier_used\"])\n",
    "        trade_symbol_used[s] = str(r[\"trade_symbol\"])\n",
    "\n",
    "    # State\n",
    "    equity = float(cfg[\"initial_capital\"])\n",
    "    fx = float(cfg[\"fx\"])\n",
    "    target_port_vol = float(cfg[\"target_portfolio_vol\"])\n",
    "    comm = float(cfg[\"commission_per_contract\"])\n",
    "    apply_comm = bool(cfg[\"apply_commissions\"])\n",
    "\n",
    "    # Positions held, expressed in TRADE contracts, keyed by signal symbol\n",
    "    prev_pos = {sym: 0 for sym in instruments}\n",
    "\n",
    "    # Logs\n",
    "    portfolio_rows = []\n",
    "    positions_rows = []\n",
    "    trades_rows = []\n",
    "\n",
    "    for t in range(1, len(common_dates)):\n",
    "        dt_prev = common_dates[t - 1]\n",
    "        dt = common_dates[t]\n",
    "\n",
    "        # --- 1) Realize P&L from positions held over (dt_prev -> dt) ---\n",
    "        pnl = 0.0\n",
    "        for sym in instruments:\n",
    "            c0 = close.at[dt_prev, sym]\n",
    "            c1 = close.at[dt, sym]\n",
    "            if pd.isna(c0) or pd.isna(c1):\n",
    "                continue\n",
    "            mult = mult_used[sym]  # TRADE multiplier\n",
    "            pnl += prev_pos[sym] * mult * fx * float(c1 - c0)\n",
    "\n",
    "        equity_before_rebal = equity\n",
    "        equity += pnl\n",
    "\n",
    "        # --- 2) Active set K_t for equal risk allocation ---\n",
    "        active_mask = (\n",
    "            (sig.loc[dt] != 0)\n",
    "            & close.loc[dt].notna()\n",
    "            & vol_ann.loc[dt].notna()\n",
    "            & (vol_ann.loc[dt] > 0)\n",
    "        )\n",
    "        active_syms = active_mask[active_mask].index.tolist()\n",
    "        K = len(active_syms)\n",
    "\n",
    "        tau_i = (target_port_vol / K) if K > 0 else 0.0\n",
    "\n",
    "        # --- 3) Compute target positions for dt -> dt+1 ---\n",
    "        new_pos = prev_pos.copy()\n",
    "\n",
    "        for sym in instruments:\n",
    "            s = int(sig.at[dt, sym])\n",
    "            c = close.at[dt, sym]\n",
    "            va = vol_ann.at[dt, sym]\n",
    "            mult = mult_used[sym]  # TRADE multiplier\n",
    "\n",
    "            if sym not in active_syms:\n",
    "                new_pos[sym] = 0\n",
    "                continue\n",
    "\n",
    "            denom = mult * float(c) * fx * float(va)\n",
    "            if denom <= 0 or not np.isfinite(denom):\n",
    "                new_pos[sym] = 0\n",
    "                continue\n",
    "\n",
    "            N_float = equity * tau_i / denom\n",
    "            N = safe_floor(N_float)\n",
    "            new_pos[sym] = int(s * N)\n",
    "\n",
    "        # --- 4) Commissions on changes (in trade contracts) ---\n",
    "        commissions = 0.0\n",
    "        if apply_comm:\n",
    "            for sym in instruments:\n",
    "                delta = new_pos[sym] - prev_pos[sym]\n",
    "                if delta != 0:\n",
    "                    commissions += abs(delta) * comm\n",
    "\n",
    "        equity -= commissions\n",
    "\n",
    "        # --- 5) Log trades ---\n",
    "        for sym in instruments:\n",
    "            delta = new_pos[sym] - prev_pos[sym]\n",
    "            if delta != 0:\n",
    "                trades_rows.append({\n",
    "                    \"date\": dt.date().isoformat(),\n",
    "                    \"signal_symbol\": sym,\n",
    "                    \"trade_symbol\": trade_symbol_used[sym],\n",
    "                    \"prev_contracts\": int(prev_pos[sym]),\n",
    "                    \"new_contracts\": int(new_pos[sym]),\n",
    "                    \"delta_contracts\": int(delta),\n",
    "                    \"commission\": float(abs(delta) * comm) if apply_comm else 0.0,\n",
    "                    \"signal\": int(sig.at[dt, sym]),\n",
    "                    \"close_signal\": float(close.at[dt, sym]) if pd.notna(close.at[dt, sym]) else np.nan,\n",
    "                    \"vol_ann\": float(vol_ann.at[dt, sym]) if pd.notna(vol_ann.at[dt, sym]) else np.nan,\n",
    "                    \"multiplier_trade\": float(mult_used[sym]),\n",
    "                    \"tau_per_instrument\": float(tau_i),\n",
    "                    \"N_float\": float(abs(equity * tau_i / (mult_used[sym] * float(close.at[dt, sym]) * fx * float(vol_ann.at[dt, sym]))))\n",
    "                              if (sym in active_syms and pd.notna(close.at[dt, sym]) and pd.notna(vol_ann.at[dt, sym]) and float(vol_ann.at[dt, sym]) > 0)\n",
    "                              else np.nan,\n",
    "                })\n",
    "\n",
    "        # Positions table: use trade symbols in columns for readability\n",
    "        pos_row = {\"date\": dt.date().isoformat()}\n",
    "        for sym in instruments:\n",
    "            col = trade_symbol_used[sym]\n",
    "            pos_row[col] = int(new_pos[sym])\n",
    "        positions_rows.append(pos_row)\n",
    "\n",
    "        # Portfolio row\n",
    "        ret = (equity - equity_before_rebal) / equity_before_rebal if equity_before_rebal != 0 else np.nan\n",
    "        portfolio_rows.append({\n",
    "            \"date\": dt.date().isoformat(),\n",
    "            \"equity\": float(equity),\n",
    "            \"pnl\": float(pnl),\n",
    "            \"commissions\": float(commissions),\n",
    "            \"return\": float(ret),\n",
    "            \"K_active\": int(K),\n",
    "            \"tau_per_instrument\": float(tau_i),\n",
    "        })\n",
    "\n",
    "        prev_pos = new_pos\n",
    "\n",
    "    portfolio_df = pd.DataFrame(portfolio_rows)\n",
    "    positions_df = pd.DataFrame(positions_rows).fillna(0)\n",
    "\n",
    "    # Ensure deterministic column order for positions: date then sorted trade symbols\n",
    "    if not positions_df.empty:\n",
    "        cols = [\"date\"] + sorted([c for c in positions_df.columns if c != \"date\"])\n",
    "        positions_df = positions_df[cols]\n",
    "\n",
    "    trades_df = pd.DataFrame(trades_rows)\n",
    "\n",
    "    return portfolio_df, positions_df, trades_df, instrument_map_df\n",
    "\n",
    "\n",
    "def write_outputs(cfg: dict, portfolio_df: pd.DataFrame, positions_df: pd.DataFrame,\n",
    "                  trades_df: pd.DataFrame, instrument_map_df: pd.DataFrame) -> None:\n",
    "    out_root: Path = cfg[\"output_root\"]\n",
    "    out_root.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # portfolio\n",
    "    portfolio_csv = out_root / \"portfolio_equity.csv\"\n",
    "    portfolio_pq  = out_root / \"portfolio_equity.parquet\"\n",
    "    portfolio_df.to_csv(portfolio_csv, index=False)\n",
    "    portfolio_df.to_parquet(portfolio_pq, index=False, compression=\"snappy\")\n",
    "\n",
    "    # positions (trade contracts)\n",
    "    positions_csv = out_root / \"positions.csv\"\n",
    "    positions_pq  = out_root / \"positions.parquet\"\n",
    "    positions_df.to_csv(positions_csv, index=False)\n",
    "    positions_df.to_parquet(positions_pq, index=False, compression=\"snappy\")\n",
    "\n",
    "    # trades\n",
    "    if not trades_df.empty:\n",
    "        trades_csv = out_root / \"trades.csv\"\n",
    "        trades_df.to_csv(trades_csv, index=False)\n",
    "\n",
    "    # mapping audit\n",
    "    map_csv = out_root / \"instrument_map.csv\"\n",
    "    instrument_map_df.to_csv(map_csv, index=False)\n",
    "\n",
    "    print(f\"Wrote outputs to: {out_root.resolve()}\")\n",
    "    print(f\"- {portfolio_csv.name}, {portfolio_pq.name}\")\n",
    "    print(f\"- {positions_csv.name}, {positions_pq.name}\")\n",
    "    if not trades_df.empty:\n",
    "        print(f\"- trades.csv\")\n",
    "    print(f\"- {map_csv.name}\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Main\n",
    "# ============================================================\n",
    "\n",
    "def main() -> None:\n",
    "    portfolio_df, positions_df, trades_df, instrument_map_df = run_backtest(CONFIG)\n",
    "    write_outputs(CONFIG, portfolio_df, positions_df, trades_df, instrument_map_df)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c5f1daf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns:\n",
      "['date', 'open', 'high', 'low', 'close', 'volume', 'delivery month', 'open interest', 'daily_prices_change_pts', 'daily_prices_change_percent', 'daily_std_full', 'annualized_std_full', 'ewma32_std', 'rolling_avg_volume_60d', 'rolling_avg_volume_20d', 'annualized_vol', 'usd_risk_volume']\n",
      "\n",
      "Dtypes:\n",
      "date                           datetime64[ns]\n",
      "open                                  float32\n",
      "high                                  float32\n",
      "low                                   float32\n",
      "close                                 float32\n",
      "volume                                float32\n",
      "delivery month                        float32\n",
      "open interest                         float32\n",
      "daily_prices_change_pts               float32\n",
      "daily_prices_change_percent           float32\n",
      "daily_std_full                        float64\n",
      "annualized_std_full                   float64\n",
      "ewma32_std                            float64\n",
      "rolling_avg_volume_60d                float64\n",
      "rolling_avg_volume_20d                float64\n",
      "annualized_vol                        float64\n",
      "usd_risk_volume                       float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "path = Path(r\"C:\\TWS API\\source\\pythonclient\\TradingIdeas\\Futures\\04a-risk_volume\\norgate_continuous\\parquet\\6A.parquet\")\n",
    "\n",
    "df = pd.read_parquet(path)\n",
    "print(\"Columns:\")\n",
    "print(df.columns.tolist())\n",
    "print(\"\\nDtypes:\")\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091d75f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_quant_lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
