{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ca85e3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 105 symbols.\n",
      "Done. Wrote per-symbol outputs to: 03-futures_price_changes\\norgate_continuous\\daily_changes\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\"\"\"\n",
    "===============================================================================\n",
    "ADD DAILY CLOSE-TO-CLOSE CHANGE (POINTS + PERCENT) TO CONTINUOUS FUTURES FILES\n",
    "===============================================================================\n",
    "\n",
    "Input options:\n",
    "1) Per-symbol files created by your notebook:\n",
    "   ./02-futures_prices/norgate_continuous/parquet/<SYMBOL>.parquet\n",
    "   ./02-futures_prices/norgate_continuous/csv/<SYMBOL>.csv\n",
    "\n",
    "2) Consolidated file (if you enabled SAVE_ALL_* in the notebook):\n",
    "   ./02-futures_prices/norgate_continuous/norgate_continuous_all.parquet (or .csv)\n",
    "\n",
    "Outputs (per symbol):\n",
    "- CSV:    OUTPUT_DIR/csv/<SYMBOL>.csv\n",
    "- Parquet OUTPUT_DIR/parquet/<SYMBOL>.parquet\n",
    "\n",
    "Appends:\n",
    "- daily_prices_change_pts\n",
    "- daily_prices_change_percent\n",
    "\"\"\"\n",
    "\n",
    "# ============================================================\n",
    "# Configuration\n",
    "# ============================================================\n",
    "\n",
    "# Set ONE of these modes:\n",
    "INPUT_MODE = \"per_symbol_dir\"   # \"per_symbol_dir\" or \"consolidated_file\"\n",
    "\n",
    "# If INPUT_MODE == \"per_symbol_dir\":\n",
    "INPUT_PARQUET_DIR = Path(\"./02-futures_prices/norgate_continuous/parquet\")\n",
    "INPUT_CSV_DIR     = Path(\"./02-futures_prices/norgate_continuous/csv\")\n",
    "PREFER_PARQUET_INPUT = True     # If True, read parquet when available, else CSV\n",
    "\n",
    "# If INPUT_MODE == \"consolidated_file\":\n",
    "CONSOLIDATED_PATH = Path(\"./02-futures_prices/norgate_continuous/norgate_continuous_all.parquet\")\n",
    "# (can also be .csv)\n",
    "\n",
    "# Output root (won't touch originals)\n",
    "OUTPUT_ROOT = Path(\"./03-futures_price_changes/norgate_continuous/daily_changes\")\n",
    "OUTPUT_CSV_DIR = OUTPUT_ROOT / \"csv\"\n",
    "OUTPUT_PARQUET_DIR = OUTPUT_ROOT / \"parquet\"\n",
    "\n",
    "# Column expectations (we’ll be robust if some are missing)\n",
    "DATE_COL = \"date\"\n",
    "CLOSE_COL = \"close\"\n",
    "\n",
    "# If your files use different names, add aliases here\n",
    "COL_ALIASES = {\n",
    "    \"delivery month\": [\"delivery month\", \"delivery_month\", \"deliverymonth\", \"contract_month\", \"delivery\"],\n",
    "    \"open interest\":  [\"open interest\", \"open_interest\", \"openinterest\", \"oi\"],\n",
    "}\n",
    "\n",
    "# ============================================================\n",
    "# Implementation\n",
    "# ============================================================\n",
    "\n",
    "def ensure_dirs():\n",
    "    OUTPUT_CSV_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    OUTPUT_PARQUET_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def normalize_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Your notebook lowercases columns; this keeps it consistent\n",
    "    df.columns = [str(c).strip().lower() for c in df.columns]\n",
    "    return df\n",
    "\n",
    "def ensure_date_column(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Per-symbol parquet/csv from the notebook uses date as index\n",
    "    if isinstance(df.index, pd.DatetimeIndex) and (DATE_COL not in df.columns):\n",
    "        df = df.reset_index()\n",
    "    if DATE_COL in df.columns:\n",
    "        df[DATE_COL] = pd.to_datetime(df[DATE_COL], errors=\"coerce\")\n",
    "        df = df.dropna(subset=[DATE_COL]).sort_values(DATE_COL)\n",
    "    return df\n",
    "\n",
    "def pick_or_create_optional_cols(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Ensure the optional columns exist (even if NaN)\n",
    "    for canonical, aliases in COL_ALIASES.items():\n",
    "        found = None\n",
    "        for a in aliases:\n",
    "            if a in df.columns:\n",
    "                found = a\n",
    "                break\n",
    "        if found is None:\n",
    "            df[canonical] = np.nan\n",
    "        elif found != canonical:\n",
    "            df = df.rename(columns={found: canonical})\n",
    "    return df\n",
    "\n",
    "def add_daily_changes(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    if CLOSE_COL not in df.columns:\n",
    "        raise ValueError(f\"Missing '{CLOSE_COL}' column. Available columns: {list(df.columns)}\")\n",
    "\n",
    "    df[CLOSE_COL] = pd.to_numeric(df[CLOSE_COL], errors=\"coerce\")\n",
    "    df[\"daily_prices_change_pts\"] = df[CLOSE_COL].diff()\n",
    "    df[\"daily_prices_change_percent\"] = df[CLOSE_COL].pct_change()\n",
    "    return df\n",
    "\n",
    "def reorder_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    desired = [\n",
    "        \"date\", \"open\", \"high\", \"low\", \"close\", \"volume\",\n",
    "        \"delivery month\", \"open interest\",\n",
    "        \"daily_prices_change_pts\", \"daily_prices_change_percent\",\n",
    "    ]\n",
    "    # Keep desired order first, then append any remaining columns\n",
    "    cols_present = [c for c in desired if c in df.columns]\n",
    "    remaining = [c for c in df.columns if c not in cols_present]\n",
    "    return df[cols_present + remaining]\n",
    "\n",
    "def read_symbol_file(sym: str) -> pd.DataFrame:\n",
    "    pq = INPUT_PARQUET_DIR / f\"{sym}.parquet\"\n",
    "    csv = INPUT_CSV_DIR / f\"{sym}.csv\"\n",
    "\n",
    "    if PREFER_PARQUET_INPUT and pq.exists():\n",
    "        df = pd.read_parquet(pq)\n",
    "    elif csv.exists():\n",
    "        df = pd.read_csv(csv)\n",
    "    elif pq.exists():\n",
    "        df = pd.read_parquet(pq)\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"No input found for {sym} in {INPUT_PARQUET_DIR} or {INPUT_CSV_DIR}\")\n",
    "\n",
    "    df = normalize_columns(df)\n",
    "    df = ensure_date_column(df)\n",
    "    return df\n",
    "\n",
    "def write_symbol_outputs(sym: str, df: pd.DataFrame):\n",
    "    out_csv = OUTPUT_CSV_DIR / f\"{sym}.csv\"\n",
    "    out_pq  = OUTPUT_PARQUET_DIR / f\"{sym}.parquet\"\n",
    "\n",
    "    df.to_csv(out_csv, index=False)\n",
    "    df.to_parquet(out_pq, index=False, compression=\"snappy\")\n",
    "\n",
    "def run_per_symbol_dir():\n",
    "    # discover symbols from parquet/csv folders\n",
    "    syms = set()\n",
    "    if INPUT_PARQUET_DIR.exists():\n",
    "        syms |= {p.stem for p in INPUT_PARQUET_DIR.glob(\"*.parquet\")}\n",
    "    if INPUT_CSV_DIR.exists():\n",
    "        syms |= {p.stem for p in INPUT_CSV_DIR.glob(\"*.csv\")}\n",
    "\n",
    "    if not syms:\n",
    "        raise RuntimeError(f\"No per-symbol files found in {INPUT_PARQUET_DIR} or {INPUT_CSV_DIR}\")\n",
    "\n",
    "    print(f\"Found {len(syms)} symbols.\")\n",
    "\n",
    "    for sym in sorted(syms):\n",
    "        df = read_symbol_file(sym)\n",
    "        df = pick_or_create_optional_cols(df)\n",
    "        df = add_daily_changes(df)\n",
    "        df = reorder_columns(df)\n",
    "        write_symbol_outputs(sym, df)\n",
    "    print(f\"Done. Wrote per-symbol outputs to: {OUTPUT_ROOT}\")\n",
    "\n",
    "def run_consolidated_file():\n",
    "    if not CONSOLIDATED_PATH.exists():\n",
    "        raise FileNotFoundError(f\"Consolidated file not found: {CONSOLIDATED_PATH}\")\n",
    "\n",
    "    if CONSOLIDATED_PATH.suffix.lower() == \".parquet\":\n",
    "        big = pd.read_parquet(CONSOLIDATED_PATH)\n",
    "    else:\n",
    "        big = pd.read_csv(CONSOLIDATED_PATH)\n",
    "\n",
    "    big = normalize_columns(big)\n",
    "    big = ensure_date_column(big)\n",
    "\n",
    "    if \"symbol\" not in big.columns:\n",
    "        raise ValueError(\"Consolidated file must contain a 'symbol' column (written by your notebook).\")\n",
    "\n",
    "    # Process each symbol independently so the diff/pct_change doesn’t bleed across symbols\n",
    "    for sym, df in big.groupby(\"symbol\", sort=True):\n",
    "        sym = str(sym).strip().upper()\n",
    "        df = df.copy()\n",
    "        df = pick_or_create_optional_cols(df)\n",
    "        df = df.sort_values(\"date\")\n",
    "        df = add_daily_changes(df)\n",
    "        df = reorder_columns(df)\n",
    "        write_symbol_outputs(sym, df)\n",
    "\n",
    "    print(f\"Done. Wrote per-symbol outputs to: {OUTPUT_ROOT}\")\n",
    "\n",
    "def main():\n",
    "    ensure_dirs()\n",
    "    if INPUT_MODE == \"per_symbol_dir\":\n",
    "        run_per_symbol_dir()\n",
    "    elif INPUT_MODE == \"consolidated_file\":\n",
    "        run_consolidated_file()\n",
    "    else:\n",
    "        raise ValueError(\"INPUT_MODE must be 'per_symbol_dir' or 'consolidated_file'\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_quant_lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
